{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d3d8b5",
   "metadata": {},
   "source": [
    "# ML models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d1337",
   "metadata": {},
   "source": [
    "## üß† BASIC MACHINE LEARNING ALGORITHMS FOR DATA SCIENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05411c",
   "metadata": {},
   "source": [
    "\n",
    "### üîπ **1. Supervised Learning**\n",
    "\n",
    "#### üìä Classification\n",
    "1. **Logistic Regression**\n",
    "   - Linear decision boundary, outputs probabilities.\n",
    "   - Regularization (L1, L2), sigmoid function.\n",
    "\n",
    "2. **Decision Trees**\n",
    "   - Interpretability, prone to overfitting, splits based on Gini/Entropy.\n",
    "\n",
    "3. **Random Forest**\n",
    "   - Ensemble of trees (bagging), reduces variance, improves generalization.\n",
    "\n",
    "4. **Gradient Boosting (e.g., XGBoost, LightGBM)**\n",
    "   - Sequentially adds models to reduce error.\n",
    "   - Highly accurate, handles missing data, feature importance.\n",
    "\n",
    "5. **Support Vector Machines (SVM)**\n",
    "   - Finds optimal hyperplane, uses kernels for non-linear data.\n",
    "   - Parameters: `C`, `gamma`, kernel type.\n",
    "\n",
    "6. **K-Nearest Neighbors (KNN)**\n",
    "   - No training, lazy learning.\n",
    "   - Distance-based classification (Euclidean, Manhattan).\n",
    "\n",
    "7. **Naive Bayes**\n",
    "   - Probabilistic classifier based on Bayes‚Äô theorem.\n",
    "   - Assumes feature independence.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà Regression\n",
    "1. **Linear Regression**\n",
    "   - Models linear relationship.\n",
    "   - Evaluate with R¬≤, MSE, residual plots.\n",
    "\n",
    "2. **Ridge & Lasso Regression**\n",
    "   - Regularization techniques:\n",
    "     - Ridge (L2): Shrinks coefficients.\n",
    "     - Lasso (L1): Shrinks and can zero out features (feature selection).\n",
    "\n",
    "3. **Polynomial Regression**\n",
    "   - Extends linear regression for curved relationships.\n",
    "\n",
    "4. **ElasticNet**\n",
    "   - Mix of L1 and L2 regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **2. Unsupervised Learning**\n",
    "\n",
    "#### üß© Clustering\n",
    "1. **K-Means Clustering**\n",
    "   - Partitions data into k clusters.\n",
    "   - Distance-based, sensitive to initialization.\n",
    "\n",
    "2. **Hierarchical Clustering**\n",
    "   - Dendrograms, doesn‚Äôt require specifying k.\n",
    "\n",
    "3. **DBSCAN**\n",
    "   - Density-based clustering, handles noise and clusters of varying shapes.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìâ Dimensionality Reduction\n",
    "1. **PCA (Principal Component Analysis)**\n",
    "   - Projects data into lower dimensions while preserving variance.\n",
    "\n",
    "2. **t-SNE / UMAP**\n",
    "   - Good for visualization of high-dimensional data.\n",
    "\n",
    "3. **Truncated SVD**\n",
    "   - For sparse matrices, used with text data (e.g., TF-IDF).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **3. Time Series & Sequence Modeling**\n",
    "1. **ARIMA / SARIMA**\n",
    "   - Classical forecasting methods (trend, seasonality).\n",
    "\n",
    "2. **Exponential Smoothing**\n",
    "   - Weighted average of past observations.\n",
    "\n",
    "3. **Recurrent Neural Networks (RNNs), LSTM, GRU**\n",
    "   - Deep learning models for sequences (if deep learning is expected).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **4. Other Important Techniques**\n",
    "- **Cross Validation** (k-fold, stratified)\n",
    "- **GridSearchCV / RandomizedSearchCV** for hyperparameter tuning\n",
    "- **Feature Selection** (Univariate, Recursive Feature Elimination)\n",
    "- **Handling Imbalanced Data** (SMOTE, class weights)\n",
    "- **Evaluation Metrics:**\n",
    "  - Classification: accuracy, precision, recall, F1, ROC-AUC\n",
    "  - Regression: MSE, RMSE, MAE, R¬≤\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ How to Prepare for Each\n",
    "| Algorithm                | What to Know | Practice |\n",
    "|-------------------------|--------------|----------|\n",
    "| Logistic Regression     | Math, sigmoid, loss function | `sklearn.linear_model.LogisticRegression` |\n",
    "| Random Forest           | Ensemble concept, overfitting control | `RandomForestClassifier`, plot feature importances |\n",
    "| Gradient Boosting       | Sequential learning, shrinkage | `XGBoost`, `LightGBM` |\n",
    "| SVM                     | Margin, kernels | `SVC` with different kernels |\n",
    "| KNN                     | Distance metrics | `KNeighborsClassifier` |\n",
    "| Linear Regression       | Cost function, assumptions | `LinearRegression`, plot residuals |\n",
    "| PCA                     | Eigenvectors, explained variance | `PCA` from `sklearn.decomposition` |\n",
    "| K-Means                 | Elbow method, silhouette score | `KMeans`, cluster plots |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bb2b4",
   "metadata": {},
   "source": [
    "## **Python libraries** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956c7c",
   "metadata": {},
   "source": [
    "### ‚úÖ **Core Python Libraries You Should Know**\n",
    "\n",
    "### üî¢ **Numerical & Matrix Operations**\n",
    "- **`NumPy`**\n",
    "  - Array creation, indexing/slicing, broadcasting\n",
    "  - Matrix multiplication (`dot`, `matmul`), reshaping, linear algebra\n",
    "\n",
    "- **`SciPy`**\n",
    "  - `scipy.stats` for statistical testing (e.g., t-tests, p-values)\n",
    "  - Optimization and numerical integration\n",
    "\n",
    "---\n",
    "\n",
    "### üêº **Data Wrangling & EDA**\n",
    "- **`pandas`**\n",
    "  - DataFrames, Series, filtering, merging, `groupby`, missing value handling\n",
    "  - `pivot`, `melt`, `.apply()`, `.map()`, `.value_counts()`\n",
    "\n",
    "- **`matplotlib` & `seaborn`**\n",
    "  - Plotting histograms, barplots, boxplots, pairplots, heatmaps\n",
    "  - EDA visualizations (correlations, distributions)\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Machine Learning & Model Evaluation**\n",
    "- **`scikit-learn`**\n",
    "  - Models: LogisticRegression, RandomForest, SVC, KMeans, PCA\n",
    "  - Pipelines, `train_test_split`, `cross_val_score`, `GridSearchCV`\n",
    "  - Preprocessing: `StandardScaler`, `MinMaxScaler`, `OneHotEncoder`\n",
    "  - Metrics: `accuracy_score`, `confusion_matrix`, `roc_auc_score`\n",
    "\n",
    "- **`xgboost`**\n",
    "  - Gradient boosting classifier and regressor\n",
    "  - Feature importance, `DMatrix` optimization\n",
    "\n",
    "- **`lightgbm`** *(optional, but nice to know)*\n",
    "  - Another gradient boosting framework (faster/lighter than XGBoost)\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Text Processing & Feature Engineering**\n",
    "- **`sklearn.feature_extraction.text.TfidfVectorizer`**\n",
    "  - Convert raw text to TF-IDF feature matrix\n",
    "- **`nltk` or `spaCy`** *(if text data is involved)*\n",
    "  - Tokenization, stopwords, stemming/lemmatization\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ **Deep Learning (maybe optional)**\n",
    "- **`tensorflow` / `keras`** or **`torch`**\n",
    "  - If they ask about ResNet, activation functions, vanishing gradients\n",
    "  - Know basic model building syntax if deep learning comes up\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Other Useful Utilities**\n",
    "- **`statsmodels`**\n",
    "  - For statistical modeling (e.g., linear regression, p-values)\n",
    "\n",
    "- **`sqlalchemy`** or **`sqlite3`**\n",
    "  - If they bring up SQL with Python integration\n",
    "\n",
    "- **`langchain`** *(mentioned in the job description)*\n",
    "  - May come up if they ask about LLMs or generative AI pipelines (you just need to know what it does)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652313c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîç Your Must-Know Set for Interview\n",
    "If you want to keep it tight and efficient, prioritize:\n",
    "```bash\n",
    "    numpy\n",
    "    pandas\n",
    "    scipy\n",
    "    matplotlib\n",
    "    seaborn\n",
    "    scikit-learn\n",
    "    xgboost\n",
    "    statsmodels\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da588a35",
   "metadata": {},
   "source": [
    "# Library Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c90a70",
   "metadata": {},
   "source": [
    "### Matrix reshaping and manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81b67334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: (3,)\n",
      "b.shape: (2, 2)\n",
      "b array:\n",
      " [[2 3]\n",
      " [1 6]]\n",
      "b.reshaped array:\n",
      " [[2 3 1 6]]\n",
      "b.reshaped array shape: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# This library is for fast matrix operations\n",
    "import numpy as np \n",
    "\n",
    "a = np.array([1,2,3]) # 1-d with 3 elments\n",
    "b = np.array([[2,3],[1,6]]) #matrix shape (2x2) 2 row x 2 columns \n",
    "\n",
    "print('a.shape:', a.shape)\n",
    "print('b.shape:', b.shape)\n",
    "\n",
    "print('b array:\\n', b)\n",
    "c = b.reshape(1,4) # reshaped it to [2,3,1,6] 1 row x 4 columns\n",
    "print('b.reshaped array:\\n', c)\n",
    "print('b.reshaped array shape:', c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c909b",
   "metadata": {},
   "source": [
    "### Stats of arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c9b0f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dot product: 14\n",
      "mean of array a: 2.0\n",
      "Std of array a: 0.816496580927726\n"
     ]
    }
   ],
   "source": [
    "d = np.dot(a,a)\n",
    "print('a dot product:', d)\n",
    "\n",
    "d = np.mean(a)\n",
    "print('mean of array a:', d)\n",
    "\n",
    "d = np.std(a)\n",
    "print('Std of array a:', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1909e5",
   "metadata": {},
   "source": [
    "### Matrix Muliplication \n",
    "Remember in matrix muliplication Colunms of Matrix 1 needs to equal to Rows of Matrix 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# d = a * b # will not work due to array a = 3 columns and b = 2 rows \n",
    "a = np.array([[2,3,4],[5,3,2]])\n",
    "b = np.array([[2,3],[5,4],[3,1]])\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "matrix1R, Matrix2R = a.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data)",
   "language": "python",
   "name": "data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
